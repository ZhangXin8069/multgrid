Start testing: Jul 16 17:54 CST
----------------------------------------------------------
1/62 Testing: StrengthTest
1/62 Test: StrengthTest
Command: "/home/zhangxin/draft/multgrid/raptor/build/raptor/tests/test_strength"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/tests
"StrengthTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from StrengthTest
[ RUN      ] StrengthTest.TestsIntests
[       OK ] StrengthTest.TestsIntests (41 ms)
[----------] 1 test from StrengthTest (41 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (41 ms total)
[  PASSED  ] 1 test.
<end of output>
Test time =   0.05 sec
----------------------------------------------------------
Test Passed.
"StrengthTest" end time: Jul 16 17:54 CST
"StrengthTest" time elapsed: 00:00:00
----------------------------------------------------------

2/62 Testing: ParStrengthTest
2/62 Test: ParStrengthTest
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_par_strength"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/tests
"ParStrengthTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_par_strength

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"ParStrengthTest" end time: Jul 16 17:54 CST
"ParStrengthTest" time elapsed: 00:00:00
----------------------------------------------------------

3/62 Testing: ParCommTest
3/62 Test: ParCommTest
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_par_comm"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/core/tests
"ParCommTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_par_comm

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"ParCommTest" end time: Jul 16 17:54 CST
"ParCommTest" time elapsed: 00:00:00
----------------------------------------------------------

4/62 Testing: TAPCommTest
4/62 Test: TAPCommTest
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_tap_comm"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/core/tests
"TAPCommTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_tap_comm

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"TAPCommTest" end time: Jul 16 17:54 CST
"TAPCommTest" time elapsed: 00:00:00
----------------------------------------------------------

5/62 Testing: ParMatrixTest
5/62 Test: ParMatrixTest
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_par_matrix"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/core/tests
"ParMatrixTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_par_matrix

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"ParMatrixTest" end time: Jul 16 17:54 CST
"ParMatrixTest" time elapsed: 00:00:00
----------------------------------------------------------

6/62 Testing: ParVectorTest
6/62 Test: ParVectorTest
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_par_vector"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/core/tests
"ParVectorTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_par_vector

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"ParVectorTest" end time: Jul 16 17:54 CST
"ParVectorTest" time elapsed: 00:00:00
----------------------------------------------------------

7/62 Testing: ParTransposeTest
7/62 Test: ParTransposeTest
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_par_transpose"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/core/tests
"ParTransposeTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_par_transpose

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"ParTransposeTest" end time: Jul 16 17:54 CST
"ParTransposeTest" time elapsed: 00:00:00
----------------------------------------------------------

8/62 Testing: ParBlockMatrixTest
8/62 Test: ParBlockMatrixTest
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_par_block_matrix"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/core/tests
"ParBlockMatrixTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_par_block_matrix

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"ParBlockMatrixTest" end time: Jul 16 17:54 CST
"ParBlockMatrixTest" time elapsed: 00:00:00
----------------------------------------------------------

9/62 Testing: ParBlockConversionTest
9/62 Test: ParBlockConversionTest
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_par_block_conversion"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/core/tests
"ParBlockConversionTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_par_block_conversion

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"ParBlockConversionTest" end time: Jul 16 17:54 CST
"ParBlockConversionTest" time elapsed: 00:00:00
----------------------------------------------------------

10/62 Testing: MatrixTest
10/62 Test: MatrixTest
Command: "/home/zhangxin/draft/multgrid/raptor/build/raptor/core/tests/test_matrix"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/core/tests
"MatrixTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from MatrixTest
[ RUN      ] MatrixTest.TestsInCore
[       OK ] MatrixTest.TestsInCore (0 ms)
[----------] 1 test from MatrixTest (0 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (0 ms total)
[  PASSED  ] 1 test.
<end of output>
Test time =   0.00 sec
----------------------------------------------------------
Test Passed.
"MatrixTest" end time: Jul 16 17:54 CST
"MatrixTest" time elapsed: 00:00:00
----------------------------------------------------------

11/62 Testing: TransposeTest
11/62 Test: TransposeTest
Command: "/home/zhangxin/draft/multgrid/raptor/build/raptor/core/tests/test_transpose"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/core/tests
"TransposeTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from MatrixTest
[ RUN      ] MatrixTest.TestsInCore
[       OK ] MatrixTest.TestsInCore (23 ms)
[----------] 1 test from MatrixTest (23 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (23 ms total)
[  PASSED  ] 1 test.
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Passed.
"TransposeTest" end time: Jul 16 17:54 CST
"TransposeTest" time elapsed: 00:00:00
----------------------------------------------------------

12/62 Testing: LaplacianSpMVTest
12/62 Test: LaplacianSpMVTest
Command: "/home/zhangxin/draft/multgrid/raptor/build/raptor/util/tests/test_spmv_laplacian"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/util/tests
"LaplacianSpMVTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from LaplacianSpMVTest
[ RUN      ] LaplacianSpMVTest.TestsInUtil
[       OK ] LaplacianSpMVTest.TestsInUtil (2 ms)
[----------] 1 test from LaplacianSpMVTest (2 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (2 ms total)
[  PASSED  ] 1 test.
<end of output>
Test time =   0.01 sec
----------------------------------------------------------
Test Passed.
"LaplacianSpMVTest" end time: Jul 16 17:54 CST
"LaplacianSpMVTest" time elapsed: 00:00:00
----------------------------------------------------------

13/62 Testing: AnisoSpMVTest
13/62 Test: AnisoSpMVTest
Command: "/home/zhangxin/draft/multgrid/raptor/build/raptor/util/tests/test_spmv_aniso"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/util/tests
"AnisoSpMVTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from AnisoSpMVTest
[ RUN      ] AnisoSpMVTest.TestsInUtil
[       OK ] AnisoSpMVTest.TestsInUtil (1 ms)
[----------] 1 test from AnisoSpMVTest (1 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (1 ms total)
[  PASSED  ] 1 test.
<end of output>
Test time =   0.00 sec
----------------------------------------------------------
Test Passed.
"AnisoSpMVTest" end time: Jul 16 17:54 CST
"AnisoSpMVTest" time elapsed: 00:00:00
----------------------------------------------------------

14/62 Testing: RandomSpMVTest
14/62 Test: RandomSpMVTest
Command: "/home/zhangxin/draft/multgrid/raptor/build/raptor/util/tests/test_spmv_random"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/util/tests
"RandomSpMVTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from RandomSpMVTest
[ RUN      ] RandomSpMVTest.TestsInUtil
[       OK ] RandomSpMVTest.TestsInUtil (0 ms)
[----------] 1 test from RandomSpMVTest (0 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (0 ms total)
[  PASSED  ] 1 test.
<end of output>
Test time =   0.00 sec
----------------------------------------------------------
Test Passed.
"RandomSpMVTest" end time: Jul 16 17:54 CST
"RandomSpMVTest" time elapsed: 00:00:00
----------------------------------------------------------

15/62 Testing: ParAddTest
15/62 Test: ParAddTest
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_par_add"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/util/tests
"ParAddTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_par_add

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"ParAddTest" end time: Jul 16 17:54 CST
"ParAddTest" time elapsed: 00:00:00
----------------------------------------------------------

16/62 Testing: ParLaplacianSpMVTest
16/62 Test: ParLaplacianSpMVTest
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_par_spmv_laplacian"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/util/tests
"ParLaplacianSpMVTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_par_spmv_laplacian

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"ParLaplacianSpMVTest" end time: Jul 16 17:54 CST
"ParLaplacianSpMVTest" time elapsed: 00:00:00
----------------------------------------------------------

17/62 Testing: ParAnisoSpMVTest
17/62 Test: ParAnisoSpMVTest
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_par_spmv_aniso"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/util/tests
"ParAnisoSpMVTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_par_spmv_aniso

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"ParAnisoSpMVTest" end time: Jul 16 17:54 CST
"ParAnisoSpMVTest" time elapsed: 00:00:00
----------------------------------------------------------

18/62 Testing: ParRandomSpMVTest
18/62 Test: ParRandomSpMVTest
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_par_spmv_random"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/util/tests
"ParRandomSpMVTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_par_spmv_random

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"ParRandomSpMVTest" end time: Jul 16 17:54 CST
"ParRandomSpMVTest" time elapsed: 00:00:00
----------------------------------------------------------

19/62 Testing: TAPLaplacianSpMVTest
19/62 Test: TAPLaplacianSpMVTest
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_tap_spmv_laplacian"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/util/tests
"TAPLaplacianSpMVTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_tap_spmv_laplacian

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"TAPLaplacianSpMVTest" end time: Jul 16 17:54 CST
"TAPLaplacianSpMVTest" time elapsed: 00:00:00
----------------------------------------------------------

20/62 Testing: TAPAnisoSpMVTest
20/62 Test: TAPAnisoSpMVTest
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_tap_spmv_aniso"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/util/tests
"TAPAnisoSpMVTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_tap_spmv_aniso

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"TAPAnisoSpMVTest" end time: Jul 16 17:54 CST
"TAPAnisoSpMVTest" time elapsed: 00:00:00
----------------------------------------------------------

21/62 Testing: TAPRandomSpMVTest
21/62 Test: TAPRandomSpMVTest
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "8" "./test_tap_spmv_random"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/util/tests
"TAPRandomSpMVTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from TAPRandomSpMVTest
[0;32m[ RUN      ] [mTAPRandomSpMVTest.TestsInUtil
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from TAPRandomSpMVTest
[0;32m[ RUN      ] [mTAPRandomSpMVTest.TestsInUtil
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from TAPRandomSpMVTest
[0;32m[ RUN      ] [mTAPRandomSpMVTest.TestsInUtil
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from TAPRandomSpMVTest
[0;32m[ RUN      ] [mTAPRandomSpMVTest.TestsInUtil
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from TAPRandomSpMVTest
[0;32m[ RUN      ] [mTAPRandomSpMVTest.TestsInUtil
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from TAPRandomSpMVTest
[0;32m[ RUN      ] [mTAPRandomSpMVTest.TestsInUtil
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from TAPRandomSpMVTest
[0;32m[ RUN      ] [mTAPRandomSpMVTest.TestsInUtil
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from TAPRandomSpMVTest
[0;32m[ RUN      ] [mTAPRandomSpMVTest.TestsInUtil
[0;32m[       OK ] [mTAPRandomSpMVTest.TestsInUtil (36 ms)
[0;32m[----------] [m1 test from TAPRandomSpMVTest (36 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (36 ms total)
[0;32m[  PASSED  ] [m1 test.
[0;32m[       OK ] [mTAPRandomSpMVTest.TestsInUtil (25 ms)
[0;32m[----------] [m1 test from TAPRandomSpMVTest (25 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (25 ms total)
[0;32m[  PASSED  ] [m1 test.
[0;32m[       OK ] [mTAPRandomSpMVTest.TestsInUtil (32 ms)
[0;32m[----------] [m1 test from TAPRandomSpMVTest (32 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[       OK ] [mTAPRandomSpMVTest.TestsInUtil (36 ms)
[0;32m[----------] [m1 test from TAPRandomSpMVTest (36 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (36 ms total)
[0;32m[  PASSED  ] [m1 test.
[0;32m[       OK ] [mTAPRandomSpMVTest.TestsInUtil (22 ms)
[0;32m[----------] [m1 test from TAPRandomSpMVTest (22 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (22 ms total)
[0;32m[  PASSED  ] [m1 test.
[0;32m[       OK ] [mTAPRandomSpMVTest.TestsInUtil (28 ms)
[0;32m[----------] [m1 test from TAPRandomSpMVTest (28 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (28 ms total)
[0;32m[  PASSED  ] [m1 test.
[0;32m[       OK ] [mTAPRandomSpMVTest.TestsInUtil (26 ms)
[0;32m[----------] [m1 test from TAPRandomSpMVTest (26 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (26 ms total)
[0;32m[  PASSED  ] [m1 test.
[0;32m[       OK ] [mTAPRandomSpMVTest.TestsInUtil (14 ms)
[0;32m[----------] [m1 test from TAPRandomSpMVTest (14 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (32 ms total)
[0;32m[  PASSED  ] [m1 test.
[0;32m[==========] [m1 test from 1 test suite ran. (15 ms total)
[0;32m[  PASSED  ] [m1 test.
<end of output>
Test time =   0.51 sec
----------------------------------------------------------
Test Passed.
"TAPRandomSpMVTest" end time: Jul 16 17:54 CST
"TAPRandomSpMVTest" time elapsed: 00:00:00
----------------------------------------------------------

22/62 Testing: ParScaleAnisoTest
22/62 Test: ParScaleAnisoTest
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "6" "./test_par_scale_aniso"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/util/tests
"ParScaleAnisoTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from ParAnisoSpMVTest
[0;32m[ RUN      ] [mParAnisoSpMVTest.TestsInUtil
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from ParAnisoSpMVTest
[0;32m[ RUN      ] [mParAnisoSpMVTest.TestsInUtil
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from ParAnisoSpMVTest
[0;32m[ RUN      ] [mParAnisoSpMVTest.TestsInUtil
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from ParAnisoSpMVTest
[0;32m[ RUN      ] [mParAnisoSpMVTest.TestsInUtil
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from ParAnisoSpMVTest
[0;32m[ RUN      ] [mParAnisoSpMVTest.TestsInUtil
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from ParAnisoSpMVTest
[0;32m[ RUN      ] [mParAnisoSpMVTest.TestsInUtil
[0;32m[       OK ] [mParAnisoSpMVTest.TestsInUtil (48 ms)
[0;32m[----------] [m1 test from ParAnisoSpMVTest (48 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (48 ms total)
[0;32m[  PASSED  ] [m1 test.
[0;32m[       OK ] [mParAnisoSpMVTest.TestsInUtil (42 ms)
[0;32m[----------] [m1 test from ParAnisoSpMVTest (42 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (42 ms total)
[0;32m[  PASSED  ] [m1 test.
[0;32m[       OK ] [mParAnisoSpMVTest.TestsInUtil (48 ms)
[0;32m[----------] [m1 test from ParAnisoSpMVTest (48 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (48 ms total)
[0;32m[  PASSED  ] [m1 test.
[0;32m[       OK ] [mParAnisoSpMVTest.TestsInUtil (37 ms)
[0;32m[----------] [m1 test from ParAnisoSpMVTest (37 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (37 ms total)
[0;32m[  PASSED  ] [m1 test.
[0;32m[       OK ] [mParAnisoSpMVTest.TestsInUtil (48 ms)
[0;32m[----------] [m1 test from ParAnisoSpMVTest (48 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (48 ms total)
[0;32m[  PASSED  ] [m1 test.
[0;32m[       OK ] [mParAnisoSpMVTest.TestsInUtil (48 ms)
[0;32m[----------] [m1 test from ParAnisoSpMVTest (48 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (48 ms total)
[0;32m[  PASSED  ] [m1 test.
<end of output>
Test time =   0.36 sec
----------------------------------------------------------
Test Passed.
"ParScaleAnisoTest" end time: Jul 16 17:54 CST
"ParScaleAnisoTest" time elapsed: 00:00:00
----------------------------------------------------------

23/62 Testing: RepartitionTest
23/62 Test: RepartitionTest
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_repartition"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/util/tests
"RepartitionTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_repartition

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"RepartitionTest" end time: Jul 16 17:54 CST
"RepartitionTest" time elapsed: 00:00:00
----------------------------------------------------------

24/62 Testing: TestRAP
24/62 Test: TestRAP
Command: "/home/zhangxin/draft/multgrid/raptor/build/raptor/ruge_stuben/tests/test_rap"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/ruge_stuben/tests
"TestRAP" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from TestRAP
[ RUN      ] TestRAP.TestsInRuge_Stuben
[       OK ] TestRAP.TestsInRuge_Stuben (39 ms)
[----------] 1 test from TestRAP (39 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (39 ms total)
[  PASSED  ] 1 test.
<end of output>
Test time =   0.04 sec
----------------------------------------------------------
Test Passed.
"TestRAP" end time: Jul 16 17:54 CST
"TestRAP" time elapsed: 00:00:00
----------------------------------------------------------

25/62 Testing: TestSplitting
25/62 Test: TestSplitting
Command: "/home/zhangxin/draft/multgrid/raptor/build/raptor/ruge_stuben/tests/test_splitting"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/ruge_stuben/tests
"TestSplitting" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from TestSplitting
[ RUN      ] TestSplitting.TestsInRuge_Stuben
[       OK ] TestSplitting.TestsInRuge_Stuben (9 ms)
[----------] 1 test from TestSplitting (9 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (9 ms total)
[  PASSED  ] 1 test.
<end of output>
Test time =   0.01 sec
----------------------------------------------------------
Test Passed.
"TestSplitting" end time: Jul 16 17:54 CST
"TestSplitting" time elapsed: 00:00:00
----------------------------------------------------------

26/62 Testing: TestInterpolation
26/62 Test: TestInterpolation
Command: "/home/zhangxin/draft/multgrid/raptor/build/raptor/ruge_stuben/tests/test_interpolation"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/ruge_stuben/tests
"TestInterpolation" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from TestInterpolation
[ RUN      ] TestInterpolation.TestsInRuge_Stuben
[       OK ] TestInterpolation.TestsInRuge_Stuben (39 ms)
[----------] 1 test from TestInterpolation (39 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (39 ms total)
[  PASSED  ] 1 test.
<end of output>
Test time =   0.04 sec
----------------------------------------------------------
Test Passed.
"TestInterpolation" end time: Jul 16 17:54 CST
"TestInterpolation" time elapsed: 00:00:00
----------------------------------------------------------

27/62 Testing: TestRugeStuben
27/62 Test: TestRugeStuben
Command: "/home/zhangxin/draft/multgrid/raptor/build/raptor/ruge_stuben/tests/test_ruge_stuben"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/ruge_stuben/tests
"TestRugeStuben" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from TestRugeStuben
[ RUN      ] TestRugeStuben.TestsInRuge_Stuben
[       OK ] TestRugeStuben.TestsInRuge_Stuben (46 ms)
[----------] 1 test from TestRugeStuben (46 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (46 ms total)
[  PASSED  ] 1 test.
<end of output>
Test time =   0.05 sec
----------------------------------------------------------
Test Passed.
"TestRugeStuben" end time: Jul 16 17:54 CST
"TestRugeStuben" time elapsed: 00:00:00
----------------------------------------------------------

28/62 Testing: TestParRAP
28/62 Test: TestParRAP
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_par_rap"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/ruge_stuben/tests
"TestParRAP" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_par_rap

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"TestParRAP" end time: Jul 16 17:54 CST
"TestParRAP" time elapsed: 00:00:00
----------------------------------------------------------

29/62 Testing: TestTAPRAP
29/62 Test: TestTAPRAP
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_tap_rap"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/ruge_stuben/tests
"TestTAPRAP" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_tap_rap

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"TestTAPRAP" end time: Jul 16 17:54 CST
"TestTAPRAP" time elapsed: 00:00:00
----------------------------------------------------------

30/62 Testing: TestParSplitting
30/62 Test: TestParSplitting
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_par_splitting"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/ruge_stuben/tests
"TestParSplitting" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_par_splitting

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"TestParSplitting" end time: Jul 16 17:54 CST
"TestParSplitting" time elapsed: 00:00:00
----------------------------------------------------------

31/62 Testing: TestTAPSplitting
31/62 Test: TestTAPSplitting
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_tap_splitting"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/ruge_stuben/tests
"TestTAPSplitting" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_tap_splitting

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"TestTAPSplitting" end time: Jul 16 17:54 CST
"TestTAPSplitting" time elapsed: 00:00:00
----------------------------------------------------------

32/62 Testing: TestParInterpolation
32/62 Test: TestParInterpolation
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_par_interpolation"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/ruge_stuben/tests
"TestParInterpolation" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_par_interpolation

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"TestParInterpolation" end time: Jul 16 17:54 CST
"TestParInterpolation" time elapsed: 00:00:00
----------------------------------------------------------

33/62 Testing: TestTAPInterpolation
33/62 Test: TestTAPInterpolation
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_tap_interpolation"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/ruge_stuben/tests
"TestTAPInterpolation" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_tap_interpolation

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"TestTAPInterpolation" end time: Jul 16 17:54 CST
"TestTAPInterpolation" time elapsed: 00:00:00
----------------------------------------------------------

34/62 Testing: TestParRugeStuben
34/62 Test: TestParRugeStuben
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_par_ruge_stuben"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/ruge_stuben/tests
"TestParRugeStuben" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_par_ruge_stuben

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"TestParRugeStuben" end time: Jul 16 17:54 CST
"TestParRugeStuben" time elapsed: 00:00:00
----------------------------------------------------------

35/62 Testing: TestTAPRugeStuben
35/62 Test: TestTAPRugeStuben
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_tap_ruge_stuben"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/ruge_stuben/tests
"TestTAPRugeStuben" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_tap_ruge_stuben

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"TestTAPRugeStuben" end time: Jul 16 17:54 CST
"TestTAPRugeStuben" time elapsed: 00:00:00
----------------------------------------------------------

36/62 Testing: TestMIS
36/62 Test: TestMIS
Command: "/home/zhangxin/draft/multgrid/raptor/build/raptor/aggregation/tests/test_mis"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/aggregation/tests
"TestMIS" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from TestMIS
[ RUN      ] TestMIS.TestsInAggregation
[       OK ] TestMIS.TestsInAggregation (4 ms)
[----------] 1 test from TestMIS (4 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (4 ms total)
[  PASSED  ] 1 test.
<end of output>
Test time =   0.01 sec
----------------------------------------------------------
Test Passed.
"TestMIS" end time: Jul 16 17:54 CST
"TestMIS" time elapsed: 00:00:00
----------------------------------------------------------

37/62 Testing: TestAggregate
37/62 Test: TestAggregate
Command: "/home/zhangxin/draft/multgrid/raptor/build/raptor/aggregation/tests/test_aggregate"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/aggregation/tests
"TestAggregate" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from TestAggregate
[ RUN      ] TestAggregate.TestsInAggregation
[       OK ] TestAggregate.TestsInAggregation (7 ms)
[----------] 1 test from TestAggregate (7 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (7 ms total)
[  PASSED  ] 1 test.
<end of output>
Test time =   0.01 sec
----------------------------------------------------------
Test Passed.
"TestAggregate" end time: Jul 16 17:54 CST
"TestAggregate" time elapsed: 00:00:00
----------------------------------------------------------

38/62 Testing: TestCandidates
38/62 Test: TestCandidates
Command: "/home/zhangxin/draft/multgrid/raptor/build/raptor/aggregation/tests/test_candidates"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/aggregation/tests
"TestCandidates" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from TestCandidates
[ RUN      ] TestCandidates.TestsInAggregation
[       OK ] TestCandidates.TestsInAggregation (11 ms)
[----------] 1 test from TestCandidates (11 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (11 ms total)
[  PASSED  ] 1 test.
<end of output>
Test time =   0.01 sec
----------------------------------------------------------
Test Passed.
"TestCandidates" end time: Jul 16 17:54 CST
"TestCandidates" time elapsed: 00:00:00
----------------------------------------------------------

39/62 Testing: TestProlongation
39/62 Test: TestProlongation
Command: "/home/zhangxin/draft/multgrid/raptor/build/raptor/aggregation/tests/test_prolongation"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/aggregation/tests
"TestProlongation" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from TestCandidates
[ RUN      ] TestCandidates.TestsInAggregation
[       OK ] TestCandidates.TestsInAggregation (15 ms)
[----------] 1 test from TestCandidates (15 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (15 ms total)
[  PASSED  ] 1 test.
<end of output>
Test time =   0.02 sec
----------------------------------------------------------
Test Passed.
"TestProlongation" end time: Jul 16 17:54 CST
"TestProlongation" time elapsed: 00:00:00
----------------------------------------------------------

40/62 Testing: TestSmoothedAggregation
40/62 Test: TestSmoothedAggregation
Command: "/home/zhangxin/draft/multgrid/raptor/build/raptor/aggregation/tests/test_smoothed_aggregation"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/aggregation/tests
"TestSmoothedAggregation" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from TestCandidates
[ RUN      ] TestCandidates.TestsInAggregation
[       OK ] TestCandidates.TestsInAggregation (40 ms)
[----------] 1 test from TestCandidates (40 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (40 ms total)
[  PASSED  ] 1 test.
<end of output>
Test time =   0.04 sec
----------------------------------------------------------
Test Passed.
"TestSmoothedAggregation" end time: Jul 16 17:54 CST
"TestSmoothedAggregation" time elapsed: 00:00:00
----------------------------------------------------------

41/62 Testing: TestParMIS
41/62 Test: TestParMIS
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_par_mis"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/aggregation/tests
"TestParMIS" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_par_mis

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"TestParMIS" end time: Jul 16 17:54 CST
"TestParMIS" time elapsed: 00:00:00
----------------------------------------------------------

42/62 Testing: TestTAPMIS
42/62 Test: TestTAPMIS
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_tap_mis"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/aggregation/tests
"TestTAPMIS" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_tap_mis

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.02 sec
----------------------------------------------------------
Test Failed.
"TestTAPMIS" end time: Jul 16 17:54 CST
"TestTAPMIS" time elapsed: 00:00:00
----------------------------------------------------------

43/62 Testing: TestParAggregate
43/62 Test: TestParAggregate
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_par_aggregate"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/aggregation/tests
"TestParAggregate" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_par_aggregate

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"TestParAggregate" end time: Jul 16 17:54 CST
"TestParAggregate" time elapsed: 00:00:00
----------------------------------------------------------

44/62 Testing: TestTAPAggregate
44/62 Test: TestTAPAggregate
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_tap_aggregate"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/aggregation/tests
"TestTAPAggregate" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_tap_aggregate

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"TestTAPAggregate" end time: Jul 16 17:54 CST
"TestTAPAggregate" time elapsed: 00:00:00
----------------------------------------------------------

45/62 Testing: TestParCandidates
45/62 Test: TestParCandidates
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_par_candidates"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/aggregation/tests
"TestParCandidates" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_par_candidates

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"TestParCandidates" end time: Jul 16 17:54 CST
"TestParCandidates" time elapsed: 00:00:00
----------------------------------------------------------

46/62 Testing: TestTAPCandidates
46/62 Test: TestTAPCandidates
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_tap_candidates"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/aggregation/tests
"TestTAPCandidates" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_tap_candidates

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"TestTAPCandidates" end time: Jul 16 17:54 CST
"TestTAPCandidates" time elapsed: 00:00:00
----------------------------------------------------------

47/62 Testing: TestParProlongation
47/62 Test: TestParProlongation
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_par_prolongation"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/aggregation/tests
"TestParProlongation" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_par_prolongation

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"TestParProlongation" end time: Jul 16 17:54 CST
"TestParProlongation" time elapsed: 00:00:00
----------------------------------------------------------

48/62 Testing: TestTAPProlongation
48/62 Test: TestTAPProlongation
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_tap_prolongation"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/aggregation/tests
"TestTAPProlongation" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_tap_prolongation

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"TestTAPProlongation" end time: Jul 16 17:54 CST
"TestTAPProlongation" time elapsed: 00:00:00
----------------------------------------------------------

49/62 Testing: TestParSmoothedAggregation
49/62 Test: TestParSmoothedAggregation
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "16" "./test_par_smoothed_aggregation"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/aggregation/tests
"TestParSmoothedAggregation" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./test_par_smoothed_aggregation

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
<end of output>
Test time =   0.03 sec
----------------------------------------------------------
Test Failed.
"TestParSmoothedAggregation" end time: Jul 16 17:54 CST
"TestParSmoothedAggregation" time elapsed: 00:00:00
----------------------------------------------------------

50/62 Testing: AMGTest
50/62 Test: AMGTest
Command: "/home/zhangxin/draft/multgrid/raptor/build/raptor/multilevel/tests/test_amg"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/multilevel/tests
"AMGTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from AMGTest
[ RUN      ] AMGTest.TestsInMultilevel
Ruge Stuben Solver:
Strength 0
Num Levels = 2
A	NRow	NCol	NNZ
0	125	125	2197
1	10	10	92

P	NRow	NCol	NNZ
0	125	10	234

Solve Phase Relative Residuals:
Res[0] = 1.000000e+00
Res[1] = 2.037608e-01
Res[2] = 7.171033e-02
Res[3] = 2.728061e-02
Res[4] = 9.801475e-03
Res[5] = 3.384610e-03
Res[6] = 1.151345e-03
Res[7] = 3.897522e-04
Res[8] = 1.317389e-04
Res[9] = 4.450790e-05
Res[10] = 1.503483e-05
Res[11] = 5.078562e-06
Res[12] = 1.715447e-06
Res[13] = 5.794448e-07
Res[14] = 1.957250e-07

Smoothed Aggregation Solver:
Strength 1
Forming S..
Num Levels = 2
A	NRow	NCol	NNZ
0	125	125	2197
1	8	8	64

P	NRow	NCol	NNZ
0	125	8	357

Solve Phase Relative Residuals:
Res[0] = 1.000000e+00
Res[1] = 2.042730e-01
Res[2] = 6.907482e-02
Res[3] = 2.567362e-02
Res[4] = 9.176896e-03
Res[5] = 3.163316e-03
Res[6] = 1.075360e-03
Res[7] = 3.639294e-04
Res[8] = 1.229918e-04
Res[9] = 4.154795e-05
Res[10] = 1.403352e-05
Res[11] = 4.739867e-06
Res[12] = 1.600886e-06
Res[13] = 5.406961e-07
Res[14] = 1.826188e-07
[       OK ] AMGTest.TestsInMultilevel (5 ms)
[----------] 1 test from AMGTest (5 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (5 ms total)
[  PASSED  ] 1 test.
<end of output>
Test time =   0.01 sec
----------------------------------------------------------
Test Passed.
"AMGTest" end time: Jul 16 17:54 CST
"AMGTest" time elapsed: 00:00:00
----------------------------------------------------------

51/62 Testing: ParAMGTest
51/62 Test: ParAMGTest
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "2" "./test_par_amg"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/multilevel/tests
"ParAMGTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from ParAMGTest
[0;32m[ RUN      ] [mParAMGTest.TestsInMultilevel
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from ParAMGTest
[0;32m[ RUN      ] [mParAMGTest.TestsInMultilevel
Num Levels = 2
A	NRow	NCol	NNZ
0	125	125	2197
1	9	9	79
Res[0] = 1.000000e+00
Res[1] = 2.678563e-02
Res[2] = 6.242691e-04
Res[3] = 1.505465e-05
Res[4] = 3.943942e-07
Res[5] = 1.446879e-08
Num Levels = 2
A	NRow	NCol	NNZ
0	125	125	2197
1	7	7	49

P	NRow	NCol	NNZ
0	125	7	355

Solve Phase Relative Residuals:
Res[0] = 1.000000e+00
Res[1] = 3.664149e-02
Res[2] = 1.567609e-03
Res[3] = 9.756314e-05
Res[4] = 5.423193e-06
Res[5] = 3.294683e-07
[0;32m[       OK ] [mParAMGTest.TestsInMultilevel (4 ms)
[0;32m[----------] [m1 test from ParAMGTest (4 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (4 ms total)
[0;32m[  PASSED  ] [m1 test.
[0;32m[       OK ] [mParAMGTest.TestsInMultilevel (4 ms)
[0;32m[----------] [m1 test from ParAMGTest (4 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (4 ms total)
[0;32m[  PASSED  ] [m1 test.
<end of output>
Test time =   0.12 sec
----------------------------------------------------------
Test Passed.
"ParAMGTest" end time: Jul 16 17:54 CST
"ParAMGTest" time elapsed: 00:00:00
----------------------------------------------------------

52/62 Testing: StencilTest
52/62 Test: StencilTest
Command: "/home/zhangxin/draft/multgrid/raptor/build/raptor/gallery/tests/test_stencil"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/gallery/tests
"StencilTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from StencilTest
[ RUN      ] StencilTest.TestsInGallery
[       OK ] StencilTest.TestsInGallery (0 ms)
[----------] 1 test from StencilTest (0 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (0 ms total)
[  PASSED  ] 1 test.
<end of output>
Test time =   0.00 sec
----------------------------------------------------------
Test Passed.
"StencilTest" end time: Jul 16 17:54 CST
"StencilTest" time elapsed: 00:00:00
----------------------------------------------------------

53/62 Testing: LaplacianTest
53/62 Test: LaplacianTest
Command: "/home/zhangxin/draft/multgrid/raptor/build/raptor/gallery/tests/test_laplacian"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/gallery/tests
"LaplacianTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from LaplacianTest
[ RUN      ] LaplacianTest.TestsInGallery
[       OK ] LaplacianTest.TestsInGallery (11 ms)
[----------] 1 test from LaplacianTest (11 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (11 ms total)
[  PASSED  ] 1 test.
<end of output>
Test time =   0.01 sec
----------------------------------------------------------
Test Passed.
"LaplacianTest" end time: Jul 16 17:54 CST
"LaplacianTest" time elapsed: 00:00:00
----------------------------------------------------------

54/62 Testing: AnisoTest
54/62 Test: AnisoTest
Command: "/home/zhangxin/draft/multgrid/raptor/build/raptor/gallery/tests/test_aniso"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/gallery/tests
"AnisoTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from AnisoTest
[ RUN      ] AnisoTest.TestsInGallery
[       OK ] AnisoTest.TestsInGallery (3 ms)
[----------] 1 test from AnisoTest (3 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (3 ms total)
[  PASSED  ] 1 test.
<end of output>
Test time =   0.01 sec
----------------------------------------------------------
Test Passed.
"AnisoTest" end time: Jul 16 17:54 CST
"AnisoTest" time elapsed: 00:00:00
----------------------------------------------------------

55/62 Testing: MatrixMarketTest
55/62 Test: MatrixMarketTest
Command: "/home/zhangxin/draft/multgrid/raptor/build/raptor/gallery/tests/test_matrix_market"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/gallery/tests
"MatrixMarketTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from AnisoTest
[ RUN      ] AnisoTest.TestsInGallery
[       OK ] AnisoTest.TestsInGallery (14 ms)
[----------] 1 test from AnisoTest (14 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (14 ms total)
[  PASSED  ] 1 test.
<end of output>
Test time =   0.02 sec
----------------------------------------------------------
Test Passed.
"MatrixMarketTest" end time: Jul 16 17:54 CST
"MatrixMarketTest" time elapsed: 00:00:00
----------------------------------------------------------

56/62 Testing: ParLaplacianTest
56/62 Test: ParLaplacianTest
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "2" "./test_par_laplacian"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/gallery/tests
"ParLaplacianTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from ParLaplacianTest
[0;32m[ RUN      ] [mParLaplacianTest.TestsInGallery
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from ParLaplacianTest
[0;32m[ RUN      ] [mParLaplacianTest.TestsInGallery
[0;32m[       OK ] [mParLaplacianTest.TestsInGallery (7 ms)
[0;32m[----------] [m1 test from ParLaplacianTest (7 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (7 ms total)
[0;32m[  PASSED  ] [m1 test.
[0;32m[       OK ] [mParLaplacianTest.TestsInGallery (7 ms)
[0;32m[----------] [m1 test from ParLaplacianTest (7 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (7 ms total)
[0;32m[  PASSED  ] [m1 test.
<end of output>
Test time =   0.13 sec
----------------------------------------------------------
Test Passed.
"ParLaplacianTest" end time: Jul 16 17:54 CST
"ParLaplacianTest" time elapsed: 00:00:00
----------------------------------------------------------

57/62 Testing: ParAnisoTest
57/62 Test: ParAnisoTest
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "2" "./test_par_aniso"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/gallery/tests
"ParAnisoTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from ParAnisoTest
[0;32m[ RUN      ] [mParAnisoTest.TestsInGallery
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from ParAnisoTest
[0;32m[ RUN      ] [mParAnisoTest.TestsInGallery
[0;32m[       OK ] [mParAnisoTest.TestsInGallery (2 ms)
[0;32m[----------] [m1 test from ParAnisoTest (2 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (2 ms total)
[0;32m[  PASSED  ] [m1 test.
[0;32m[       OK ] [mParAnisoTest.TestsInGallery (2 ms)
[0;32m[----------] [m1 test from ParAnisoTest (2 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (2 ms total)
[0;32m[  PASSED  ] [m1 test.
<end of output>
Test time =   0.11 sec
----------------------------------------------------------
Test Passed.
"ParAnisoTest" end time: Jul 16 17:54 CST
"ParAnisoTest" time elapsed: 00:00:00
----------------------------------------------------------

58/62 Testing: ParMatrixMarketTest
58/62 Test: ParMatrixMarketTest
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "2" "./test_par_matrix_market"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/gallery/tests
"ParMatrixMarketTest" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from ParAnisoTest
[0;32m[ RUN      ] [mParAnisoTest.TestsInGallery
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from ParAnisoTest
[0;32m[ RUN      ] [mParAnisoTest.TestsInGallery
[0;32m[       OK ] [mParAnisoTest.TestsInGallery (15 ms)
[0;32m[----------] [m1 test from ParAnisoTest (15 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (15 ms total)
[0;32m[  PASSED  ] [m1 test.
[0;32m[       OK ] [mParAnisoTest.TestsInGallery (16 ms)
[0;32m[----------] [m1 test from ParAnisoTest (16 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (16 ms total)
[0;32m[  PASSED  ] [m1 test.
<end of output>
Test time =   0.13 sec
----------------------------------------------------------
Test Passed.
"ParMatrixMarketTest" end time: Jul 16 17:54 CST
"ParMatrixMarketTest" time elapsed: 00:00:00
----------------------------------------------------------

59/62 Testing: TestCG
59/62 Test: TestCG
Command: "/home/zhangxin/draft/multgrid/raptor/build/raptor/krylov/tests/test_cg"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/krylov/tests
"TestCG" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from CGTest
[ RUN      ] CGTest.TestsInKrylov
160 Iteration required to converge
2 Norm of Residual: 7.86214e-05

Residuals[0] = 8.657722e+00
[       OK ] CGTest.TestsInKrylov (46 ms)
[----------] 1 test from CGTest (46 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (46 ms total)
[  PASSED  ] 1 test.
<end of output>
Test time =   0.05 sec
----------------------------------------------------------
Test Passed.
"TestCG" end time: Jul 16 17:54 CST
"TestCG" time elapsed: 00:00:00
----------------------------------------------------------

60/62 Testing: TestBiCGStab
60/62 Test: TestBiCGStab
Command: "/home/zhangxin/draft/multgrid/raptor/build/raptor/krylov/tests/test_bicgstab"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/krylov/tests
"TestBiCGStab" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from BiCGStabTest
[ RUN      ] BiCGStabTest.TestsInKrylov
111 Iterations Required to Converge.
2 Norm of Residual: 0.000083385186201

[       OK ] BiCGStabTest.TestsInKrylov (64 ms)
[----------] 1 test from BiCGStabTest (64 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (64 ms total)
[  PASSED  ] 1 test.
<end of output>
Test time =   0.07 sec
----------------------------------------------------------
Test Passed.
"TestBiCGStab" end time: Jul 16 17:54 CST
"TestBiCGStab" time elapsed: 00:00:00
----------------------------------------------------------

61/62 Testing: TestParCG
61/62 Test: TestParCG
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "1" "./test_par_cg"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/krylov/tests
"TestParCG" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from ParCGTest
[0;32m[ RUN      ] [mParCGTest.TestsInKrylov
160 Iteration required to converge
2 Norm of Residual: 7.86214e-05

[0;32m[       OK ] [mParCGTest.TestsInKrylov (50 ms)
[0;32m[----------] [m1 test from ParCGTest (50 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (50 ms total)
[0;32m[  PASSED  ] [m1 test.
<end of output>
Test time =   0.14 sec
----------------------------------------------------------
Test Passed.
"TestParCG" end time: Jul 16 17:54 CST
"TestParCG" time elapsed: 00:00:00
----------------------------------------------------------

62/62 Testing: TestParBiCGStab
62/62 Test: TestParBiCGStab
Command: "/home/zhangxin/openmpi/bin/mpirun" "-n" "1" "./test_par_bicgstab"
Directory: /home/zhangxin/draft/multgrid/raptor/build/raptor/krylov/tests
"TestParBiCGStab" start time: Jul 16 17:54 CST
Output:
----------------------------------------------------------
[0;32m[==========] [mRunning 1 test from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from ParBiCGStabTest
[0;32m[ RUN      ] [mParBiCGStabTest.TestsInKrylov
100 Iteration required to converge
2 Norm of Residual: 7.59052e-05

[0;32m[       OK ] [mParBiCGStabTest.TestsInKrylov (66 ms)
[0;32m[----------] [m1 test from ParBiCGStabTest (66 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m1 test from 1 test suite ran. (66 ms total)
[0;32m[  PASSED  ] [m1 test.
<end of output>
Test time =   0.15 sec
----------------------------------------------------------
Test Passed.
"TestParBiCGStab" end time: Jul 16 17:54 CST
"TestParBiCGStab" time elapsed: 00:00:00
----------------------------------------------------------

End testing: Jul 16 17:54 CST
